# -*- coding: utf-8 -*-




'''
----------------- Introduction of PathGen script ---------------
Version 0.1
Author WenJun


What we can do?
-- generate a reaction path network
-- select important site
-- nothing else can be done


Required moudle
python 3.9
pandas
matplotlib
networkx
rdkit


Example 
-- Input file path of bond.rate.tab generated by Chemtrayzer
-- Input the initial reactant, target product
-- wait long long time and return a network graph
















--------------------------------------------------------------
'''

''' >>>>>>> Main Programs >>>>>> '''

import matplotlib.pyplot as plt
import pandas as pd
import networkx as nx



'''
初始化列表, pandas筛选后仍然保留了原来的数据表，美发进行新的操作
处理Default.rate.tab文件
处理1：
添加新列 N_ave = 正反应-逆反应数,  k = 正反应-逆反应速率，option='yes',筛选N_ave>0的反应
df1 = 对df处理后的新列表，重置index
处理2：
拆分反应列表，删除左右有相同化学式的项
处理3 ：
合并去重后相同的行合相反的行。并处理N_ave，
                    #1 A+B -> C+D                 |       # A+B -> C+D 
                    #2 A+B -> C+D 合并N_ave       |       # C+D -> A+B  # 相减N
'''
def ini_RecLst (file_name,reactNum) :
    
    ''' # 处理1：去除逆反应 '''
    df = pd.read_csv(file_name,sep=';',engine='c',low_memory=False)
    N = df[df.index%2==0]['N'].values - df[df.index%2==1]['N'].values 
    k = df[df.index%2==0]['k'].values - df[df.index%2==1]['k'].values
    
    df.loc[df.index%2==0,'N_ave'] =  N
    df.loc[df.index%2==1,'N_ave'] = -N
    df.loc[df.index%2==0,'k_ave'] =  k
    df.loc[df.index%2==1,'k_ave'] = -k
    
    # note: reactNum,筛选N_ave>0的反应
    df1 = df[df.N_ave > reactNum]
    df1 = df1.reset_index(drop=True)
    df1[['R','P']] = df1["Formula's"].str.split('->',expand=True)   # 拆分反应列表

    # a = df1[df1['R'].str.contains('C12H26')]
    # 处理2： 简化等式左右重复的元素
    rowR = []
    rowP = []
    a = df1["R"].str.replace(' ','').str.split('+')
    b = df1["P"].str.replace(' ','').str.split('+')    
    
    for i, row in enumerate(a) : 
        res = set(a[i]).intersection(set(b[i]))
        if res :  # 待修正：左边有a，在右边查看是否也有a，是则两端都删除一个a，否则下一个。
            a[i] = [aa for aa in a[i] if aa not in list(res) ]
            b[i] = [xx for xx in b[i] if xx not in list(res) ]
            rowR.append(a[i])
            rowP.append(b[i])
        else :
            rowR.append(a[i])
            rowP.append(b[i])
    
    
    ''' # 处理3 ： 删除无效行： 重复的行 '''
    del_idx = []   
    re_idx = []                             # 避免重复检索，记录下检索过的行
    for ii in range(len(rowR)) :
        if rowR[ii] in [[],None] and (rowP[ii] in [[],None]):
            del_idx.append(ii)
            re_idx.append(ii)
            continue
        else:
            rowR[ii] = sorted(rowR[ii])      # 先排序，保证顺序一致后再比较
            rowP[ii] = sorted(rowP[ii]) 
            rowR[ii] = list(set(rowR[ii]))  # # 删除元素
            rowP[ii] = list(set(rowP[ii]))

    for ii in range(len(rowR)) :  

        
        if (ii not in re_idx) :              
            for jj in range(len(rowR)):
                if (ii != jj) & (jj not in re_idx) :
                    #1 A+B -> C+D 
                    #2 A+B -> C+D 合并N_ave
                    if (rowR[ii] == rowR[jj]) & (rowP[ii] ==rowP[jj]) :
                        # +N
                        df1.loc[ii]['N'] = df1.loc[ii]['N'] + df1.loc[jj]['N']
                        # del lineNo
                        del_idx.append (jj)
                        
                        # retrieve No
                        re_idx.append (ii)
                        re_idx.append (jj)
                    
                    # A+B -> C+D
                    # C+D -> A+B
                    # 相减N
                    if (rowR[ii] == rowP[jj]) & (rowP[ii] ==rowR[jj]) :
                        # +N
                         mm = df1.loc[ii]['N'] - df1.loc[jj]['N']
                        
                         if mm > 0 :
                            df1.loc[ii]['N'] = mm
                            del_idx.append (jj)

                         if mm < 0 :
                            df1.loc[jj]['N'] = abs(mm) 
                            del_idx.append (ii)
                        
                         if mm == 0 :
                            # del lineNo
                            del_idx.append (ii)
                            del_idx.append (jj)
                            
                            # retrieve No
                         re_idx.append (ii)
                         re_idx.append (jj)

    df1['Sim_R'] =  rowR
    df1['Sim_P'] =  rowP
    df1['Sim_R'] = df1['Sim_R'].astype(str) # 指定这两列为str，否则不能去重
    df1['Sim_P'] = df1['Sim_P'].astype(str)
    df1 = df1.drop(df1.index[del_idx]) 
    # xxx = df1.groupby(['Sim_R','Sim_P'], sort=False)['N_ave','N'].sum().reset_index() # 合并重复行，并求合
    df1 = df1.reset_index(drop=True)                   
    return df1
        
def SelEle (node, edge, elements):                  # node = sps   edge = edges
    nd = []
    ed = []

    for e in elements :
        for i in node :
            if e in i :
                nd.append (i)                   # 筛选

        for j in edge :
            if (e in j[0]) or (e in j[1]) :
                ed.append (tuple(j))
    nd=list(set(nd))
    ed=list(set(ed))
    return nd, ed

def Sim_netwk (edge) :
    Lst = []
    lst = pd.DataFrame(edge, columns=['R','P','N'])
    lst = lst.groupby(['R','P'])['N'].sum().reset_index()
    Lst = lst.apply(lambda x: tuple(x), axis=1).values.tolist() # 每行转tuple

    # ------ 旧方法 遍历--------
    # 合并所有相同的反应，并累计N
    # 在edges边列表中，嵌套循环，比较i与j是否相同，是，则累计N，添加i,记录已添加的i在record列表中
    # 对i+1进行比较时，先检查i+1是否在record中；
    # N初始值为0，全部循环后，若未找到重复的，则添加i和对应的N
    
    # temp = []    
    # for index, row in lst.iterrows() : 
    #     a = [row["R"], row["P"]]
    #     b = int (row['N'])
    #     c = [a,b]
    #     temp.append(c)

    # record  = []    
    # for i in range(len(temp)) :
    #     N = 0
    #     if temp[i][0] in record :
    #         continue
    #     else :
    #         for j in range(len(temp)) :
    #             if temp[j][0] in record :
    #                 continue
    #             if i == j :
    #                 continue
    #             else :
    #                 if temp[i][0] == temp[j][0]:
    #                     n = int(temp[i][1]) + int(temp[j][1])
    #                     N = N+n
    #         n = 0
    #         if N == 0 :
    #             N = temp[i][1]
                    
    #     l = [temp[i][0][0], temp[i][0][1], N]
    #     ll = [temp[i][0][0], temp[i][0][1]]
    #     record.append(ll)
    #     Lst.append(tuple(l))
    return Lst

def Net_Simp (edge) :  
    ''' 构建[（a,b）,n1]的 [(b,a),n2]列表，
    if (a,b) == (b,a),则判断n1,n2大小，
    if n1>n2,  新列表t = [（a,b）,n1-n2]
    注意：要建立一个record[]列表以记录已经比较过的行，
    例如，
    A                       B
    [['CO', 'CHO'], 59] 	 [['CO', 'CHO'], 57]
    [['CHO', 'CO'], 57] 	 [['CHO', 'CO'], 59]
    B是A 的反列表，遍历时会对['CO', 'CHO'], 59]和['CHO', 'CO'], 57] 
    都检查，这样会造成两次重复的对比，且n的值相反，造成错误，
    因此检查['CO', 'CHO'], 59]时，要记录['CHO', 'CO']于record中
    这样，只要再遍历时检查A中元素是否再record中，即可避免错误的遍历
    '''
    # 复制相反的DateFrame,标记R*，合并两个DateFrame，去重加和，筛选出R的行
    # lst = pd.DataFrame(edge, columns=['R','P','N'])
    # lst_v = pd.DataFrame()
    # lst['label'] = 'R'
    # lst_v['R'] = lst['P']
    # lst_v['P'] = lst['R']
    # lst_v['N'] = -lst['N']
    # lst_v['label'] = 'R*'
    # lst = pd.concat([lst,lst_v]).reset_index(drop=True)
    # lst= lst.groupby(['R','P']) ['N'].sum().reset_index()
    # Lst = lst.apply(lambda x: tuple(x), axis=1).values.tolist() # 每行转tuple
    
    temp = []
    temp_re = []
    sim = []
    for i in edge :
        a = [ i[0], i[1] ]
        b = [a, i[2]]
        temp.append (b)
        
        a_r = [ i[1], i[0] ]
        b_r = [ a_r, i[2]]
        temp_re.append (b_r)

    record = []
    for i in temp :
        n = 0
        if i[0] not in record :
            for j in temp_re :
                
                if i[0] == j [0] :
                    n = 1
                    if int(i[1]) > int(j[1]) :
                        a = [i[0][0], i[0][1], int(i[1]) -int(j[1]) ]
                        aa = [i[0][1], i[0][0]]
                        sim.append(a)
                        record.append(aa)
                        break
                    if int(i[1]) < int(j[1]) :
                        a = [i[0][1], i[0][0], int(j[1]) -int(i[1]) ]
                        aa = [i[0][0], i[0][1]]
                        sim.append(a)
                        record.append(aa)
                        break
                    if i[1] == j[1] :
                        break
            if n == 0 :
                a = [ i[0][0], i[0][1], i[1] ]
                sim.append(a)
                
    sim_lst = pd.DataFrame(sim, columns=['R','P','N'])
    sim_lst =sim_lst.drop_duplicates()
    
    ed = sim_lst.apply(lambda x: tuple(x), axis=1).values.tolist() # 每行转tuple
    nd = list(set([nn for n  in ed for nn in n[:2]]))
     
    return  nd, ed
                                                    # content_txt = lst
def Ini_NodEdge (content_txt, elements, option ) :   # elements = ['C','H'] option = 'net'
    rx = []   
    px = []
    edges = []
    sm = content_txt['N_ave'].sum() 
    R = content_txt["Sim_R"].str.replace("[", "").str.replace("]", "").str.split(', ')
    P = content_txt["Sim_P"].str.replace("[", "").str.replace("]", "").str.split(', ')
    # 节点数
    sps1 = [ss for s in R for ss in  s]
    sps2 = [ss for s in R for ss in  s] 
    sps = list(set(sps1+sps2))
    print(f'节点数为{len(sps)}')
    for i in range(len(R)) :
    # edges
        for rr in R[i] : 
            for pp in P[i] : 
                if rr == pp :
                    continue
                else :
                    edges.append(tuple([rr,pp,int(content_txt.loc[i,"N_ave"])]))  
    # Function 筛选元素
    # nodes, edges = SelEle(sps, edges, elements)
    nodes = sps
    edges = Sim_netwk (edges)
    if 'net' in option :
        # Function network去重
        nodes, edges = Net_Simp (edges)
        
    return nodes, edges
            
def Optim (start,edge,dd,ww) : # 横坐标间距dd=5
    # 在反应列表中，利用pandas 查找反应物CO2和对应的产物，利用while i < len(Rlst) 循环中可改变Rlst的性质，
    # 查找CO2对应的产物，更新Rlst = Plst,并记录已查找过的反应物R在record中，下次循环时，先检查
    # 搜索出来的P是否在record中，若是，则下一个。

            # Plst = lst.loc[lst['R'].isin([R])] # 找Ri对应产物Pi
            # P = Plst['P']
            
    # >>>>>>>> 赋予节点坐标，
    # 循环中，将每层循环的P添加到 C[]中，生成反应物顺序列表
    # [
      # [a]
      # [b,c]
      # [d,e,f]
    # ]
    # 循环，根据层id和每层列表id赋予坐标。横坐标为层id，列坐标为1/2（每层列表长度）+元素id实现中心分布
    # 避免同层之间的连线看不清，x坐标+ p = p * (-1)，同层x坐标交错分布
    
    lst = pd.DataFrame(edge, columns=['R','P','N']) # edge = Edges start=  "'C5H12'"
    X = [start]
    C = [[start]]
    record = []
    Rlst = [start]
    B = Rlst     # 当返回的反应列表为[]，说明已经没有产物可以找到
    
    ff = open ('reac_path.txt','w+')
    
    print ('>>>>>>>>>>>> Nodes Layer >>>>>>>>>>',file = ff)
    while Rlst : # wihle 是可改变判断条件的，但for不可以
        A = []  # 当前行，所有R反应物对应的所有P产物     
        x = []
        i = 0
        print (Rlst,file = ff)
        
        while i < len(Rlst) : # 遍历反应物Ri
    
            R = Rlst[i]
            Plst = lst.loc[lst['R'].isin([R])]  # 找Ri对应产物Pi
            P = Plst['P'] 
            x.append(Plst)                      # 记录当前列表，方便后续查询N
                    
            i += 1
            
            for r in list(P) :      # 遍历Pi产物，开始进行筛选
    
                if r in record :    # record用来记录所有第一次重复检索的
                    continue
                
                ''' # 
                if (r not in record) and (r in B) : # 决定是否要闭环，
                                    # 取消(r not in record) 和A.append(r)
                    A.append(r)     # 实现不闭环：检索过但确实第一次发现重复，即在此时产生闭合
                    record.append(r)
                '''
                
                if r in B :
                    record.append(r)
                if (r not in record) and (r not in B) : 
                    A.append(r)
            B = B + list(P)             # B 是所有检索过的元素
            
        Rlst = list(set(A))             
        C.append(Rlst)
        X.append(x)   
    
    ff.close()
    # 判断相邻两行的每两元素之间是否存在连接关系，用C_Deep_Sim_Edge检查
    # 定义节点坐标，x坐标应该交错
    
    node = []
    scal = {}
    p=1
    for im in range(len(C)-1) :
        for jm in range(len(C[im])) :
            p = p * (-1)
            t = [im+p/dd, jm-len(C[im])/2]
            scal[C[im][jm]]=tuple(t)
            node.append (C[im][jm])
            
            
    # 改进边的关系，只让上下级相连
    tm = []
    for ix in range(len(C)-1) :
        a = C[ix]
        b = C[ix+1]
        for xi in a :
            for xj in b :
                tt = [xi,xj]
                tm.append (tt)
        
    
    for ix in C :
        for jx in ix :
            for kx in ix :
                
                tt = [jx,kx]
                tm.append (tt)
    
    ed,edw = [],[]
    for xc in edge :
        tc = [xc[0],xc[1]]
        if tc in tm :
            ed.append(xc)
    
    for xd in edge :
        if xd[2] > ww :
            edw.append(xd)
            
    return node, ed, scal, edw

def Netgraph (NODES, EDGES, pos, Ecolor, start, end,w, option) :
            
    G1 = nx.DiGraph()
    G1.add_nodes_from(NODES)
    # G1.add_edges_from(EDGES)
    G1.add_weighted_edges_from (EDGES)
    labels = nx.get_edge_attributes(G1,w)

    nx.draw(
            
            G1,
            node_color = 'lightblue',             # 顶点颜色
            edge_color =  Ecolor,                  # 边的颜色
            with_labels = True,                   # 显示顶点标签
            # with_labels = False,
            font_size = 8,                        # 文字大小
            font_weight = 'bold',
            node_size = 200,                      # 顶点大小
            style ='solid',
            font_family = 'sans-serif' ,
            node_shape="o",
            alpha = 0.5,
            cmap=plt.cm.Blues, 
            # bbox=dict(facecolor="skyblue", edgecolor='black', boxstyle='round,pad=0.2'))
            
            pos = pos
            
            )

    # 起点，终点
    nx.draw_networkx_edge_labels(G1,pos,edge_labels=labels, font_color='c')        
    # nx.draw_networkx_nodes(G1,pos,nodelist=[start],node_color='red')
    # nx.draw_networkx_nodes(G1,pos,nodelist=[end],node_color='yellow')

        
    if 'allpath' in option :
        ff = open ('reac_path.txt','a+') 
            
            
        # > 最长路径
        minWPath2 = nx.dijkstra_path(G1, source=start, target=end)
        print (file = ff)
        print ('>>>>>>>>>> Reaction path >>>>>>>>>>>>>',file = ff)
        print ('Longest path is :', list(minWPath2),file = ff)
        
        
        # 最短路径， 
        # 将所有边的权重变为倒数，排除n=0的，构建 G2
        
        Eds_s = []
        for edw in EDGES :
            if edw[2] == 0 :
                continue
            if edw[2] != 0 :
                eds = [ edw[0], edw[1] ,1/edw[2] ]
            Eds_s.append(eds)
    
        
        G2 = nx.DiGraph()
        G2.add_nodes_from(NODES)
        
        G2.add_weighted_edges_from (Eds_s)
        labels = nx.get_edge_attributes(G2,'weight')
        
        
        # > 最短加权路径
        
        minWPath1 = nx.dijkstra_path(G2, source=start, target=end)
        print ('Shortest weight path is :', list(minWPath1),file = ff)
        
        
        # > 所有最短路径
        
        Gu = G2.to_undirected()
        path = nx.all_shortest_paths(Gu, source=start, target= end)
        print ('All shortest path is :', list(path),file = ff)
        
        
        # > 所有可能的路径

        alpath = []
        for path in nx.all_simple_paths(G2, source=start, target=end):
            alpath.append(path)
        
        ed_len = []
        reac_len = []
        path_len = []
        for i in range(len(alpath)):
            total_len = 0
            nod_len = []
            for j in range(len(alpath[i])-1):
                source, target = alpath[i][j], alpath[i][j+1]
                edge = G1[source][target]
                length = edge['weight']          #<--- Get the weight
                nod_len.append (length)     # each edge len in this reaction path
            reac_len.append (sum(nod_len))
            ed_len.append(nod_len)
            path_len.append(len(alpath[i]))

        reac_path_lst = pd.DataFrame(alpath)
        reac_path_lst['each_edge_len'] = ed_len # edge len in each pair nodes
        reac_path_lst['N'] = reac_len           # reaction[i] length
        reac_path_lst['path_len'] = path_len
        reac_path_lst['path_rate'] = reac_path_lst['N']/reac_path_lst['path_len']
        
        # 找到反应数量最大的路径们，从中选出反应节点数最少的，
        # 应该就是最快的反应路径了
        # 反应生成CH4O的用时与生成多少个CH4O来决定反应路径的快慢
        # here, using reaction number/nodes number to indicate the 
        # path rate
        
        lst_order = reac_path_lst.sort_values(by=['path_rate'], axis=0, ascending=False)
        order = lst_order['path_rate']
        idx = list(order.index.values)
        s_top = idx[0:11]
        reac_path_lst.iloc[s_top]

        
        print (file = ff)
        print ('>>>>>>>>>> top 10  path :reaction number/nodes >>>>>>>>>>>>',file = ff) 
        
        reac_path_lst.iloc[s_top].to_csv("reac_path.csv",index=False)

        #设置value的显示长度为200，默认为50
        pd.set_option('max_colwidth',200)
        #显示所有列，把行显示设置成最大
        pd.set_option('display.max_columns', None)
        #显示所有行，把列显示设置成最大
        pd.set_option('display.max_rows', None)
        
        ff.close()
        
    else :  # 透明度
        pass
    if 'save' in option:  # transparent = True,
        plt.savefig(figpath,
                    dpi=700,format='tif',figsize=(24,10),
                    bbox_inches = 'tight' ,transparent = True)
    plt.show()
    
def Find_Reax (content, start, end) :
    # content = lst 
    # end = 'CHO2' 
    # a0.to_csv('CHO2.csv')
    # Find reaction :
    Reation = content["Formula's"].str.split('->',expand=True)       # 分列
    Reation = pd.concat([content,Reation],axis=1,ignore_index=True)
    
    Reation.columns=[
                'R<ID>', "S<ID>'s", "Formula's", 
                'SMILES', 'k', 'klo', 'kup', 'N','k_ave','R','P']
    
    #显示所有列,
    pd.set_option('display.max_columns', 10000000)
    pd.set_option('display.max_rows', 10000000)
   
    a0 = Reation[Reation['P'].str.contains(end)]
    a = Reation[Reation['R'].str.contains(start)]
    b = a[a['P'].str.contains(end)]
    
    return a0, b

def Simles2Struc (smil,filename) :
    from rdkit import Chem
    from rdkit.Chem import AllChem
    smiles = smil
    mol = AllChem.AddHs(Chem.MolFromSmiles(smiles))
    AllChem.EmbedMolecule(mol)
    AllChem.MMFFOptimizeMolecule(mol)
    Chem.MolToMolFile(mol,'./'+filename)


    
''' # Main menu # '''    

'''必须执行 '''

file_name = r'D:\zh\JP-10\REAXFF\JP-10-2200\bond\bondanalysis\2200-10-5\Default.rate.tab'
figpath = r'D:\zh\JP-10\图片\\'+'Network0-2200-1e-5.tif'

# ---------
lst = ini_RecLst(file_name,0)                                           #1 初始化列表   -- option = 1 : N_ave >= 1, -1 为全部
Nodes, Edges = Ini_NodEdge(lst, ['C','H'], 'net')                       #2 生成去重的节点和边 -- elements = ['C','H'] 包含C,H的化学式
Optim_Node, Optim_Edge, Optim_pos, wEdge  = Optim("'CH4'",Edges,6,5) #3 节点坐标布局，-- dd =5 横坐标间距；非逆级，非闭环连接关系，得到产物列表，
                                                                        # --ww=5,显示超过5的权重， 在def中的X列表可查看，会输出反应节点层级聊表
'''画图 '''
# 画出未经优化的图，这里是含有逆级和闭环的反应
# C_Deep_Sim_Node, C_Deep_Sim_Edge, 节点坐标， 边的颜色，起始物，终点物'CadetBlue'
Netgraph (Optim_Node, Optim_Edge, Optim_pos,  
          'grey', "'CH4'", "'H2'", ' ',['nosave'])
Netgraph (Optim_Node, wEdge, Optim_pos,  
          'red', "'CH4'", "'H2'", 'weight',['save']) # 'save' 保存图片， 'allpath' : # 求最短路径 
# weight,Optim_Edge替换wEdge

# 优化的反应关系图,"yes",是否计算两点间的全部路径， based on raction layer
# Netgraph (Optim_Node, Optim_Edge, Optim_pos, 'blue',"'C10H16'","'C2H4'", "yes")

# ------------------------------------------------------------------------------


# draw mole structure 

'''
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
# from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules
from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions

gpNods = pd.read_excel ( './vacancy_1500K/result/bond/v_bond_spicies.xlsx')
for i in list(C_Deep_Sim_Node) :
    mol = Chem.MolFromSmiles(gpNods.loc[0][i])
    Draw.MolToFile(
        mol,  # mol对象
        './vacancy_1500K/result/bond/chemstru/'+i+'.png',  # 图片存储地址
        size=(300, 300), 
        kekulize=True, 
        wedgeBonds=True, 
        imageType=None, 
        fitImage=False, 
        options=None, 
    )














# Reaction_list finding shorest path
def Find_Reax_onebyone (content, start, rec_chem) :
    # 重新整理列表，删除化学式重复的反应
#    content = lst_N, start = ['CO2']  , rec_chem = ['CO2'] 
    Reac_lst = content["Formula's"].str.split('->',expand=True)       # 分列
    Reac_lst = pd.concat([content,Reac_lst],axis=1,ignore_index=True)
    Reac_lst.columns=[
                'R<ID>', "S<ID>'s", "Formula's", 
                'SMILES', 'k', 'klo', 'kup', 'N','k_ave','R','P']
    for it in start :
        prds = Reac_lst[Reac_lst['R'].str.contains(it)].sort_values(by='N', ascending=False)
        p_list = [ im.replace(' ','').split('+') for im in list(prds['P']) ]

        temp = [ im.replace(' ','').split('+') for im in list(prds['R']) ] # 此次检索的反应式中所有反应物
        tar = [ t for t in p_list if (t not in rec_chem) and ('C' in it)]

        rec_chem.extend(temp)
        
        if tar :
            
            return prds,tar,rec_chem
        else :
            print ('No reaction')
    
    rec_chem = [xx for x in rec_chem for xx in x ]
    rec_chem = list(set(rec_chem))
    
    return prds,tar,rec_chem
# 初始值
tar = ['CO2']  
rec_chem = tar
# 一步一步找路径
prds,tar,rec_chem = Find_Reax_onebyone (lst_N, tar, rec_chem)
        

        
    
# x = Reaction_list_shorest_path(lst_N,'CO2',2)

#得到最短路径，寻找想要的反应信息, 查看reacx
all_to_target, Reax = Find_Reax ( lst, 'CH2O2', 'CH2O' )
print(Reax)



# 讲反应信息中的smiles转为sdf结构
# SDF = Simles2Struc('smil', '.sdf')





        
'''
     
     

        





 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



##########################
# df = pd.read_csv(file_name, sep= ';')
# data['name'].str.split('|',expand=True)       # 分裂
# lst.loc[lst['N'].isin([0])]                   # 筛选
# for index, row in sim_lst.iterrows() :        # 遍历
#     a = [row["R"], row["P"], int (row['N'])]
# index = b.index.tolist()                      # 获得行索引
# Reation = pd.concat([Reation,lst],axis=1,ignore_index=True) # 横向拼接，axis=1为纵向拼接

# nx.from_pandas_adjacency(dfAdj)





